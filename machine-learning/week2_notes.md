# Week 2

## multivariate linear regression

### Multiple Features

### Gradient Descent for Multiple Variables

### Gradient Descent in Practice I - Feature Scaling

### gradient descent in practice II - Learning Rate

- For sufficiently small $$\alpha$$, $$J(\theta)$$ should decrease on every iteration.
- But if $$\alpha$$ is too small, gradient descent can be slow to converge.

Summary:

- If $$\alpha$$ is too small: slow convergence
- If $$\alpha$$ is too large: $$J(\theta)$$ may not decrease on every iteration; may not converge.

### Features and Ploynomial Regression

## Compting Parametaers Analytically

### Normal Equation

Method to solve for $$\theta$$ analytically



